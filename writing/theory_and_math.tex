\section*{Neighborhood Model Representation}
Let's imagine a very small city consisting of four blocks. We can
represent this tiny town as a network where two blocks are connected
if they face the same street. In this representation, blocks that are
kitty corner are not directly connected. We'll index the blocks as
$1$, $2$, $3$, and $4$.

\begin{figure}[h]
\centering
\tikz{
\draw[help lines] (0,0) grid (2,2);
\node at (0.5, 0.5) {3} ;
\node at (1.5, 1.5) {2} ;
\node at (0.5, 1.5) {1} ;
\node at (1.5, 0.5) {4} ;
}
\end{figure}

\begin{figure}[h]
\centering

\tikz{ %
  \node[latent] (1) {$1$} ; %
  \node[latent, below left=of 1] (2) {$2$} ; %
  \node[latent, below right=of 1] (3) {$3$} ; %
  \node[latent, below left=of 3] (4) {$4$} ; %
  \edge[-] {2,3} {1} ; %
  \edge[-] {2,3} {4} ; %
}

\end{figure}

Suppose that, in our city, there are two neighborhoods. Each block
belongs to either one or the other of these neighborhoods. Neighboring
blocks that are similar are more apt to belong to the same
neighborhood and neighboring blocks that are different are more apt to
belong to different neighborhoods.

We want similar, neighboring blocks to belong to the same
neighborhood. One way to formalize this desire is to score every
possible pattern of neighborhood labels in such a way that our
preferred patterns have the best score.

First, we need some more precise terms. Let the two neighborhoods be
called $0$ and $1$. Every block belongs to either neighborhood $0$ or
$1$. We will denote this membership as $y_i$, so that $y_1=0$ is
equivalent to saying that block $1$ belongs to the $0$
neighborhood. Let the similarity between blocks $i$ and $j$ be called
$\phi_{i,j}$.

\begin{figure}[!h]
\centering

\tikz{ %
  \node[latent] (1) {$y_1$} ; %
  \node[latent, below left=of 1] (2) {$y_2$} ; %
  \node[latent, below right=of 1] (3) {$y_3$} ; %
  \node[latent, below left=of 3] (4) {$y_4$} ; %
  \factor[below left=of 1] {1-2} {$\phi_{1,2}$} {} {} ;
  \factor[below right=of 1] {1-3} {$\phi_{1,3}$} {} {} ;
  \factor[below right=of 2] {2-4} {$\phi_{2,4}$} {} {} ;
  \factor[below left=of 3] {3-4} {$\phi_{3,4}$} {} {} ;
  \factoredge[-] {1} {1-2} {2} ; %
  \factoredge[-] {1} {1-3} {3} ; %
  \factoredge[-] {2} {2-4} {4} ; %
  \factoredge[-] {3} {3-4} {4} ; %
  %\edge[-] {2,3} {4} ; %
}

\end{figure}


Let a particular pattern of assignment of blocks to neighborhoods
be called $\mathbf{y}$.  The score of $\mathbf{y}$ will be
$\operatorname{E}(\mathbf{y})$\footnote{$\operatorname{E}$ as in
  energy not expectation} which will take the following form:

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i,j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j,\phi_{i,j})
\end{align}

Where $<i,j>$ indexes a pair of neighboring blocks, and where $i < j$.
$\mathcal{N}$ are all these indices of neighboring blocks.

\begin{equation}
\epsilon_{i,j}(y_i,y_j,\phi_{i,j}) = \begin{cases}
  0 &y_i = y_j \\
  \phi_{i,j} &y_i \neq y_j
\end{cases}
\end{equation}

Suppose that we want our preferred neighborhood assignment to have the
lowest score. If our similarity measures $\phi_{i,j}$ are positive
when blocks are similar and negative when blocks are different, then
we will encourage similar, neighboring blocks to belong to the same
neighborhood. 

Let our city have the following $\phi$'s.

\begin{align*}
&\phi_{1,2} = 1 \\
&\phi_{1,3} = -1 \\
&\phi_{2,4} = -1 \\
&\phi_{3,4} = 1
\end{align*} 

\noindent
If we now score every possible pattern of neighborhood labels, we will
find that the lowest scoring assignments are ones that put blocks $1$
and $2$ in one neighborhood and $3$ and $4$ in the other (Table
\ref{table:lowest}, Table \ref{table:energy}). By choosing the right
\phi$'s we can have made our preferred pattern have the lowest scores.

\begin{table}
\centering
  \begin{tabular}{cc}
      \tikz{ %
        \node[latent] (1) {$y_1$} ; %
        \node[latent, below left=of 1] (2) {$y_2$} ; %
        \node[latent, fill=black, below right=of 1] (3) {\textcolor{white}{$y_3$}} ; %
        \node[latent, fill=black, below left=of 3] (4) {\textcolor{white}{$y_4$}} ; %
        \factor[below left=of 1] {1-2} {$1$} {} {} ;
        \factor[below right=of 1] {1-3} {$-1$} {} {} ;
        \factor[below right=of 2] {2-4} {$-1$} {} {} ;
        \factor[below left=of 3] {3-4} {$1$} {} {} ;
        \factoredge[-] {1} {1-2} {2} ; %
        \factoredge[-] {1} {1-3} {3} ; %
        \factoredge[-] {2} {2-4} {4} ; %
        \factoredge[-] {3} {3-4} {4} ; %
      } 
    &
      \tikz{ %
        \node[latent, fill=black] (1) {\textcolor{white}{$y_1$}} ; %
        \node[latent, fill=black, below left=of 1] (2) {\textcolor{white}{$y_2$}} ; %
        \node[latent, below right=of 1] (3) {$y_3$} ; %
        \node[latent, below left=of 3] (4) {$y_4$} ; %
        \factor[below left=of 1] {1-2} {$1$} {} {} ;
        \factor[below right=of 1] {1-3} {$-1$} {} {} ;
        \factor[below right=of 2] {2-4} {$-1$} {} {} ;
        \factor[below left=of 3] {3-4} {$1$} {} {} ;
        \factoredge[-] {1} {1-2} {2} ; %
        \factoredge[-] {1} {1-3} {3} ; %
        \factoredge[-] {2} {2-4} {4} ; %
        \factoredge[-] {3} {3-4} {4} ; %
      } 
    \\
  \end{tabular}
  \caption{Preferred Assignment: If $y_i = 0$, the block is colored
    white. If $y_i = 1$, the block is black.}
  \label{table:lowest}
\end{table}

\begin{table}[h]
\input{energy_table.tex}
\caption{Scores of Neighborhood Assignments}
\label{table:energy}
\end{table}

For small networks, can check the scores for each individual pattern
to see if our preferred pattern really has the lowest score. This
exhaustive strategy becomes quickly impractical for larger
cities. With two possible neighborhoods, the number of possible
assignments is $2^N$ where N is the number of blocks. This
permutational explosion means that for even small cities, we cannot
possibly check every possible neighborhood assignment in human-scale
time. 

\subsection*{Network Methods}
Fortunately, researchers, largely in the field of computer vision,
have developed methods to quickly find the lowest scoring assignment
for problems like ours. This work traces back to a 1986 paper by
Greig, Porteous, and Sehult, where they demonstrated that finding
lowest scoring assignment for a two neighborhood case was equivalent
to solving the problem of finding the minimum cut of a
graph.\cite{greig_exact_1989}

In that paper, the authors set up an scoring problem that is slightly
different than ours. In addition to terms that depended upon pairs of
neighbors, their score also included terms for individual nodes.

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_i\epsilon_i(y_i) + \sum_{<i
  j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j,\phi_{i,j})
\end{align}

\noindent
The node terms were of one of two forms, either

\begin{equation}
\epsilon_{i}(y_i) = \begin{cases}
  0 &y_i = 0 \\
  \phi_{i} \geq 0 &y_i = 1
\end{cases}
\end{equation}

\noindent
or 

\begin{equation}
\epsilon_{i}(y_i) = \begin{cases}
  \phi_{i} \geq 0 &y_i = 0 \\
  0 &y_i = 1
\end{cases}
\end{equation}

\noindent
They had the same form of $\epsilon_{i,j}$

\begin{equation}
\epsilon_{i,j}(y_i,y_j,\phi_{i,j}) = \begin{cases}
  0 &y_i = y_j \\
  \phi_{i,j} &y_i \neq y_j
\end{cases}
\end{equation}

\noindent
but unlike us, $\phi_{i,j} \geq 0$ for all $<i, j>$ in $\mathcal{N}$. 

One of their networks could look like this

\begin{figure}[!h]
\centering

\tikz{ %
  \node[latent] (1) {$y_1$} ; %
  \node[latent, below left=of 1] (2) {$y_2$} ; %
  \node[latent, below right=of 1] (3) {$y_3$} ; %
  \node[latent, below left=of 3] (4) {$y_4$} ; %
  \factor[below left=of 1] {1-2} {$1$} {} {} ;
  \factor[below right=of 1] {1-3} {$1$} {} {} ;
  \factor[below right=of 2] {2-4} {$1$} {} {} ;
  \factor[below left=of 3] {3-4} {$1$} {} {} ;
  \factoredge[-] {1} {1-2} {2} ; %
  \factoredge[-] {1} {1-3} {3} ; %
  \factoredge[-] {2} {2-4} {4} ; %
  \factoredge[-] {3} {3-4} {4} ; %
  %\edge[-] {2,3} {4} ; %
}

\end{figure}

Based on this undirected network, Greig and his co-authors constructed
a special, directed network where every edge has a cost associated
with it. For every variable $y_i$ in the undirected network, the
directed network contains a node $z_i$.  For every edge $(y_i, y_j)$ in the
undirected graph, they introduced a directed edge from $z_i$ to $z_j$ and
another from $z_j$ to $z_i$ with associated costs of $\phi_{i,j}$.

In addition to the original nodes, they also introduced two special
nodes: a source node called $s$ and a target, or sink, node called
$t$. If $\epsilon_i(0) = 0$ then there would be a directed edge from
$s$ to the $i$th node. If $\epsilon_i(1) = 0$ then there would be
directed edge from the $i$th node to $t$. Networks like this are
called s-t networks.

Suppose that $\epsilon_1(1)=2$, $\epsilon_2(0)=1$, $\epsilon_3(1)=3$,
and $\epsilon_4(0)=3$. The corresponding directed graph is shown in
Figure \ref{fig:directed}. 

\begin{figure}[!h]
\centering

\begin{tikzpicture}[auto, >= stealth', shorten >= 1pt, node
    distance=2cm, thick]
\tikzset{vertex/.style = {shape=circle, draw, minimum size = 1.5em}}
\tikzset{Dedge/.style = {->}}
\tikzset{Uedge/.style = {<->}}

\node[vertex, above=of 1] (0) {$s$} ; %
\node[vertex] (1) {$y_1$} ; %
\node[vertex, below left=of 1] (2) {$y_2$} ; %
\node[vertex, below right=of 1] (3) {$y_3$} ; %
\node[vertex, below left=of 3] (4) {$y_4$} ; %
\node[vertex, below=of 4] (5) {$t$} ; %
\path
  (0) edge [Dedge] node {2} (1)
  (0) edge [Dedge, bend left] node[right] {3} (3)
  (1) edge [Uedge] node {1} (2) 
  (1) edge [Uedge] node {1} (3) 
  (2) edge [Uedge] node {1} (4) 
  (3) edge [Uedge] node {1} (4) 
  (2) edge [Dedge, bend right] node[left] {1} (5)
  (4) edge [Dedge] node {3} (5)  ;

\end{tikzpicture}
\caption{S-T Network}
\label{fig:directed}
\end{figure}

We must now introduce some terms. First, to `cut' a network is to
remove edges from the network so that the network is split into two
unconnected, smaller networks. Second, an `s-t cut' is a cut on a s-t
network that causes the source node and target node to end up in
separate, unconnected networks. Finally, a `minimum cut' is an s-t cut
where the sum of cost associated with the removed edges is as small as
any other possible s-t cut. 

In our example, removing the edges $(z_2, t)$ and $(z_4, t)$ is an s-t
cut, but not the minimum cut. The minimum cut would be removing edges
$(z_1, z_2)$ and $(z_3, z_4)$ (Figure \ref{fig:mincut}).

\begin{figure}[!h]
\centering

\begin{tikzpicture}[auto, >= stealth', shorten >= 1pt, node
    distance=2cm, thick]
\tikzset{vertex/.style = {shape=circle, draw, minimum size = 1.5em}}
\tikzset{Dedge/.style = {->}}
\tikzset{Uedge/.style = {<->}}

\node[vertex, above=of 1] (0) {$s$} ; %
\node[vertex] (1) {$y_1$} ; %
\node[vertex, below left=of 1] (2) {$y_2$} ; %
\node[vertex, below right=of 1] (3) {$y_3$} ; %
\node[vertex, below left=of 3] (4) {$y_4$} ; %
\node[vertex, below=of 4] (5) {$t$} ; %
\path
  (0) edge [Dedge] node {2} (1)
  (0) edge [Dedge, bend left] node[right] {3} (3)
  (1) edge [Uedge] node {1} (3) 
  (2) edge [Uedge] node {1} (4) 
  (2) edge [Dedge, bend right] node[left] {1} (5)
  (4) edge [Dedge] node {3} (5)  ;

\end{tikzpicture}
\caption{Minimum Cut}
\label{fig:mincut}
\end{figure}

Every s-t cut can be mapped onto a pattern of neighborhood block
labels by letting $y_i = 1$ if node $z_i$ ends up in the $t$
subnetwork and $y_i = 0$ otherwise. Greig and his coauthors
demonstrated that the minimum s-t cut their constructed s-t network
always corresponded to the minimum scoring pattern of block
assignment.

Because computer scientists have known how to solve the minimum cut
problem swiftly since the 1950s, this meant that a large class of
network configurations problems were now tractable. Instead of
checking every possible pattern out of exponentially possibilities, we
can now find the best scoring pattern directly and
quickly.\cite{ford_maximal_1956}

\subsubsection*{Submodularity}
Greig's original result only applied to scoring functions 
where blocks could belong to only one of two classes and where the
scoring function had an important property called submodularity.  A
submodular scoring function would be one where $\epsilon_{i,j}(0,0) +
\epsilon_{i,j}(1,1) \leq \epsilon_{i,j}(0,1) + \epsilon_{i,j}(1,0)$
for all $i,j$ in $\mathcal{N}$. Our scoring function is decisively not
submodular because we want it to cost more to put together very
dissimilar neighboring blocks than to assign them to them to different
neighborhoods. That is, for very dissimilar blocks we want it to hold
that $\epsilon_{i,j}(0,0) + \epsilon_{i,j}(1,1) \boldsymbol{>}
\epsilon_{i,j}(0,1) + \epsilon_{i,j}(1,0)$.

Since we don't have a submodular scoring functions and often will have
more than two neighborhood names, we can't use the minimum cuts
algorithm directly to find the lowest scoring neighborhood
pattern. However, since the 1980s computers scientists have developed
other graph cutting methods that allow us to find approximate
solutions to multi-label non-submodular scoring problems like ours
\cite{something}.

\section*{Learning similarities}
In the previous section, we showed how we could construct a scoring
function so that a preferred pattern of block assignments would have
the best score. Now we will turn to how, given a preferred assignment,
we can learn such a scoring function. 

We will first make our lives easier by only considering simple, linear
similarity terms. As before, the scoring function will be

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i,j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j,\phi_{i,j})
\end{align}

\noindent
and

\begin{equation}
\epsilon_{i,j}(y_i,y_j,\phi_{i,j}) = \begin{cases}
  0 &y_i = y_j \\
  \phi_{i,j} &y_i \neq y_j
\end{cases}
\end{equation}

\noindent
but now, let the similarity term $\phi_{i,j}$ be the weighted sum of
observed similarities between block $i$ and $j$. 

\begin{align}
\phi_{i,j} = w_0 + w_1s_{1,i,j} + w_2s_{2,i,j} + ... + w_ns_{n,i,j}
\end{align} 

\noindent
An observed inter-block similarity could be absolute difference in
population, or dummy variable indicating whether the blocks are
separated by a railroad, or similar.

\noindent

With this form, learning a scoring function means finding weights
that give a better score to our preferred pattern of neighborhood
assignments than any other pattern. In other words, we want to find
some weights, $\mathbf{w}$, that solves this system of linear
equations:

\begin*{equations}
\operatorname{E}(\mathbf{y}_1, \mathbf{s}, \mathbf{w})
\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \\
\operatorname{E}(\mathbf{y}_2, \mathbf{s}, \mathbf{w})
\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w})
\ldots \\
\operatorname{E}(\mathbf{y}_{M-1}, \mathbf{s}, \mathbf{w})
\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w})
\operatorname{E}(\mathbf{y}_{M}, \mathbf{s}, \mathbf{w})
\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w})
\end{equations}

Where \mathbf{y}^* is our preferred pattern, M is the number of
possible neighborhood patterns, and $\mathbf{s}$ are the similarities
between blocks.

\subsubsection*{Specifying a unique solution}
In order to use some convenient machinery, we will modify the learning
problem somewhat. These modifications will only act to specify a
particular solution to the system of inequalities. Any solution to the
modified problems is also a solution to the system of inequalities and
if there is solution to the system of inequalities there will be a
solution to the modified problems.

First, observe that if a set of weights gives the preferred assignment
a lower score than any other assignment, there is some difference
between the score of the preferred assignment and the score of the
next best scoring assignment. Call this difference the 'margin. Our
first modification to the learning problem is that we now seek the
weights that give our preferred assignment the largest margin.

\begin{align}
&\argmax_{\mathbf{w}} \mathbf{\gamma} \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq \gamma\\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in the set of
  possible neighborhood assignments}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align}

This still does not specify a unique set of weights. To do that, we
constrain the sizes of the weights. We could do this by limiting the
sum of the absolute values of the weights. A will be momentarily
clear, more convenient choice is to require that $\sqrt{\sum_i^M w_i^2
  = 1}$. The learning problem is now:

%
\begin{align*}
&\argmax_{\mathbf{w}:\sqrt{\sum_i^M w_i^2 = 1}=1} \mathbf{\gamma} \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq \gamma\\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in the set of
  possible neighborhood assignments}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align*}
%

With these two modifications, the learning problem is now a quadratic
program, a well known class of constrained optimization
problems.\footnote{The problem is easier to recognize as a quadratic
  program in it's dual, canonical form:
%
\begin{align*}
&\argmin_{\mathbf{w}} \frac{1}{2}||\mathbf{w}||^2 \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq 1 \\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in the set of
  possible neighborhood assignments}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align*}
}

Unfortunately, we cannot directly use an off-the-shelf quadratic
program solver, because the number of constraints will typically be
too large. We require every possible pattern to have a higher
score than our target pattern, and the number of possible patterns
grows exponentially with the number of blocks.

Instead, we will solve a similar problem that takes advantage of the
network cutting algorithm but which can be solved
quickly. The algorithm for solving this problem is typically called
the ``structured support vector machine.''\cite{szummer_learning_2008}
 

\subsubsection*{Structured Support Vector Machine}
First, we initialize the weights to some starting value, create an
empty set of constraints $\mathcal{K}$, and set a counter $i$ to $0$.
\begin{enumerate}
\item Use a minimum cuts algorithm to find the neighborhood pattern
  that has a lower score than any pattern given the current weights.
  Call this pattern $\mathbf{y}_i$ and add it to the constraint
  set $\mathcal{K}$.

\item Update the weights by solving the quadratic program: 
%
\begin{align*}
&\argmax_{\mathbf{w}:\sqrt{\sum_i^M w_i^2 = 1}=1} \mathbf{\gamma} \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq \gamma\\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in } \mathcal{K}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align*}
%

\item If the weights changed in the previous step, set $i = i + 1$ and
  go to step 1. If the weights did not change, stop the routine.
\end{enumerate}

If we find a set of weights that gives our target pattern a lower
energy then any pattern in the constraint set $\mathcal{K}$, then we
are assured that our target pattern has a lower score than any other
possible pattern. If another pattern had a lower score given the final
set of weights, then it would have been added to $\mathcal{K}$ in step
1, and our optimization routine would have continued. 

With this structured support vector machine, we have the means of
efficiently learning weights that give a targeted pattern a better
score than any other pattern, if those weights exist. 

There is a similar, but more general form for finding the weights that
give the target assignment a lower score or for finding the weights
that minimize the difference between the score of the target pattern
and the lowest scoring pattern. There are also a tweaks that penalize
the scores of patterns that empirically diverge from the target
pattern, and which can help the algorithm learn better weights for
predicting new patterns. 

The details of these extensions are best found in x, y or z. For our
current purposes, we use one the extensions, where the slack variables
are weighted by block-by-block Hamming loss.
