\section*{Neighborhood Model Representation}
Let's imagine a very small city consisting of four blocks. We can
represent this tiny town as a network where two blocks are connected
if they face the same street. In this representation, blocks that are
kitty corner are not directly connected. We'll index the blocks as
$1$, $2$, $3$, and $4$.

\begin{figure}[h]
\centering
\tikz{
\draw[help lines] (0,0) grid (2,2);
\node at (0.5, 0.5) {3} ;
\node at (1.5, 1.5) {2} ;
\node at (0.5, 1.5) {1} ;
\node at (1.5, 0.5) {4} ;
}
\end{figure}

\begin{figure}[h]
\centering

\tikz{ %
  \node[latent] (1) {$1$} ; %
  \node[latent, below left=of 1] (2) {$2$} ; %
  \node[latent, below right=of 1] (3) {$3$} ; %
  \node[latent, below left=of 3] (4) {$4$} ; %
  \edge[-] {2,3} {1} ; %
  \edge[-] {2,3} {4} ; %
}

\end{figure}

Suppose that, in our city, there are two neighborhoods. Each block
belongs to either one or the other of these neighborhoods. Neighboring
blocks that are similar are more apt to belong to the same
neighborhood and neighboring blocks that are different are more apt to
belong to different neighborhoods.

We want similar, neighboring blocks to belong to the same
neighborhood. One way to formalize this desire is to score every
possible pattern of neighborhood labels in such a way that our
preferred patterns have the best score.

First, we need some more precise terms. Let the two neighborhoods be
called $0$ and $1$. Every block belongs to either neighborhood $0$ or
$1$. We will denote this membership as $y_i$, so that $y_1=0$ is
equivalent to saying that block $1$ belongs to the $0$
neighborhood. Let the similarity between blocks $i$ and $j$ be called
$\phi_{i,j}$.

\begin{figure}[!h]
\centering

\tikz{ %
  \node[latent] (1) {$y_1$} ; %
  \node[latent, below left=of 1] (2) {$y_2$} ; %
  \node[latent, below right=of 1] (3) {$y_3$} ; %
  \node[latent, below left=of 3] (4) {$y_4$} ; %
  \factor[below left=of 1] {1-2} {$\phi_{1,2}$} {} {} ;
  \factor[below right=of 1] {1-3} {$\phi_{1,3}$} {} {} ;
  \factor[below right=of 2] {2-4} {$\phi_{2,4}$} {} {} ;
  \factor[below left=of 3] {3-4} {$\phi_{3,4}$} {} {} ;
  \factoredge[-] {1} {1-2} {2} ; %
  \factoredge[-] {1} {1-3} {3} ; %
  \factoredge[-] {2} {2-4} {4} ; %
  \factoredge[-] {3} {3-4} {4} ; %
  %\edge[-] {2,3} {4} ; %
}

\end{figure}


Let a particular pattern of assignment of blocks to neighborhoods
be called $\mathbf{y}$.  The score of $\mathbf{y}$ will be
$\operatorname{E}(\mathbf{y})$\footnote{$\operatorname{E}$ as in
  energy not expectation} which will take the following form:

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i,j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j,\phi_{i,j})
\end{align}

Where $<i,j>$ indexes a pair of neighboring blocks, and where $i < j$.
$\mathcal{N}$ are all these indices of neighboring blocks.

\begin{equation}
\epsilon_{i,j}(y_i,y_j,\phi_{i,j}) = \begin{cases}
  0 &y_i = y_j \\
  \phi_{i,j} &y_i \neq y_j
\end{cases}
\end{equation}

Suppose that we want our preferred neighborhood assignment to have the
lowest score. If our similarity measures $\phi_{i,j}$ are positive
when blocks are similar and negative when blocks are different, then
we will encourage similar, neighboring blocks to belong to the same
neighborhood. 

Let our city have the following $\phi$'s.

\begin{align*}
&\phi_{1,2} = 1 \\
&\phi_{1,3} = -1 \\
&\phi_{2,4} = -1 \\
&\phi_{3,4} = 1
\end{align*} 

\noindent
If we now score every possible pattern of neighborhood labels, we will
find that the lowest scoring assignments are ones that put blocks $1$
and $2$ in one neighborhood and $3$ and $4$ in the other (Table
\ref{table:lowest}, Table \ref{table:energy}). By choosing the right
\phi$'s we can have made our preferred pattern have the lowest scores.

\begin{table}
\centering
  \begin{tabular}{cc}
      \tikz{ %
        \node[latent] (1) {$y_1$} ; %
        \node[latent, below left=of 1] (2) {$y_2$} ; %
        \node[latent, fill=black, below right=of 1] (3) {\textcolor{white}{$y_3$}} ; %
        \node[latent, fill=black, below left=of 3] (4) {\textcolor{white}{$y_4$}} ; %
        \factor[below left=of 1] {1-2} {$1$} {} {} ;
        \factor[below right=of 1] {1-3} {$-1$} {} {} ;
        \factor[below right=of 2] {2-4} {$-1$} {} {} ;
        \factor[below left=of 3] {3-4} {$1$} {} {} ;
        \factoredge[-] {1} {1-2} {2} ; %
        \factoredge[-] {1} {1-3} {3} ; %
        \factoredge[-] {2} {2-4} {4} ; %
        \factoredge[-] {3} {3-4} {4} ; %
      } 
    &
      \tikz{ %
        \node[latent, fill=black] (1) {\textcolor{white}{$y_1$}} ; %
        \node[latent, fill=black, below left=of 1] (2) {\textcolor{white}{$y_2$}} ; %
        \node[latent, below right=of 1] (3) {$y_3$} ; %
        \node[latent, below left=of 3] (4) {$y_4$} ; %
        \factor[below left=of 1] {1-2} {$1$} {} {} ;
        \factor[below right=of 1] {1-3} {$-1$} {} {} ;
        \factor[below right=of 2] {2-4} {$-1$} {} {} ;
        \factor[below left=of 3] {3-4} {$1$} {} {} ;
        \factoredge[-] {1} {1-2} {2} ; %
        \factoredge[-] {1} {1-3} {3} ; %
        \factoredge[-] {2} {2-4} {4} ; %
        \factoredge[-] {3} {3-4} {4} ; %
      } 
    \\
  \end{tabular}
  \caption{Preferred Assignment: If $y_i = 0$, the block is colored
    white. If $y_i = 1$, the block is black.}
  \label{table:lowest}
\end{table}

\begin{table}[h]
\input{energy_table.tex}
\caption{Scores of Neighborhood Assignments}
\label{table:energy}
\end{table}

For small networks, can check the scores for each individual pattern
to see if our preferred pattern really has the lowest score. This
exhaustive strategy becomes quickly impractical for larger
cities. With two possible neighborhoods, the number of possible
assignments is $2^N$ where N is the number of blocks. This
permutational explosion means that for even small cities, we cannot
possibly check every possible neighborhood assignment in human-scale
time. 

\subsection*{Network Methods}
Fortunately, researchers, largely in the field of computer vision,
have developed methods to quickly find the lowest scoring assignment
for problems like ours. This work traces back to a 1986 paper by
Greig, Porteous, and Sehult, where they demonstrated that finding
lowest scoring assignment for a two neighborhood case was equivalent
to solving the problem of finding the minimum cut of a
graph.\cite{greig_exact_1989}

In that paper, the authors set up an scoring problem that is slightly
different than ours. In addition to terms that depended upon pairs of
neighbors, their score also included terms for individual nodes.

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_i\epsilon_i(y_i) + \sum_{<i
  j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j,\phi_{i,j})
\end{align}

\noindent
The node terms were of one of two forms, either

\begin{equation}
\epsilon_{i}(y_i) = \begin{cases}
  0 &y_i = 0 \\
  \phi_{i} \geq 0 &y_i = 1
\end{cases}
\end{equation}

\noindent
or 

\begin{equation}
\epsilon_{i}(y_i) = \begin{cases}
  \phi_{i} \geq 0 &y_i = 0 \\
  0 &y_i = 1
\end{cases}
\end{equation}

\noindent
They had the same form of $\epsilon_{i,j}$

\begin{equation}
\epsilon_{i,j}(y_i,y_j,\phi_{i,j}) = \begin{cases}
  0 &y_i = y_j \\
  \phi_{i,j} &y_i \neq y_j
\end{cases}
\end{equation}

\noindent
but unlike us, $\phi_{i,j} \geq 0$ for all $<i, j>$ in $\mathcal{N}$. 

One of their networks could look like this

\begin{figure}[!h]
\centering

\tikz{ %
  \node[latent] (1) {$y_1$} ; %
  \node[latent, below left=of 1] (2) {$y_2$} ; %
  \node[latent, below right=of 1] (3) {$y_3$} ; %
  \node[latent, below left=of 3] (4) {$y_4$} ; %
  \factor[below left=of 1] {1-2} {$1$} {} {} ;
  \factor[below right=of 1] {1-3} {$1$} {} {} ;
  \factor[below right=of 2] {2-4} {$1$} {} {} ;
  \factor[below left=of 3] {3-4} {$1$} {} {} ;
  \factoredge[-] {1} {1-2} {2} ; %
  \factoredge[-] {1} {1-3} {3} ; %
  \factoredge[-] {2} {2-4} {4} ; %
  \factoredge[-] {3} {3-4} {4} ; %
  %\edge[-] {2,3} {4} ; %
}

\end{figure}

Based on this undirected network, Greig and his co-authors constructed
a special, directed network where every edge has a cost associated
with it. For every variable $y_i$ in the undirected network, the
directed network contains a node $z_i$.  For every edge $(y_i, y_j)$ in the
undirected graph, they introduced a directed edge from $z_i$ to $z_j$ and
another from $z_j$ to $z_i$ with associated costs of $\phi_{i,j}$.

In addition to the original nodes, they also introduced two special
nodes: a source node called $s$ and a target, or sink, node called
$t$. If $\epsilon_i(0) = 0$ then there would be a directed edge from
$s$ to the $i$th node. If $\epsilon_i(1) = 0$ then there would be
directed edge from the $i$th node to $t$. Networks like this are
called s-t networks.

Suppose that $\epsilon_1(1)=2$, $\epsilon_2(0)=1$, $\epsilon_3(1)=3$,
and $\epsilon_4(0)=3$. The corresponding directed graph is shown in
Figure \ref{fig:directed}. 

\begin{figure}[!h]
\centering

\begin{tikzpicture}[auto, >= stealth', shorten >= 1pt, node
    distance=2cm, thick]
\tikzset{vertex/.style = {shape=circle, draw, minimum size = 1.5em}}
\tikzset{Dedge/.style = {->}}
\tikzset{Uedge/.style = {<->}}

\node[vertex, above=of 1] (0) {$s$} ; %
\node[vertex] (1) {$y_1$} ; %
\node[vertex, below left=of 1] (2) {$y_2$} ; %
\node[vertex, below right=of 1] (3) {$y_3$} ; %
\node[vertex, below left=of 3] (4) {$y_4$} ; %
\node[vertex, below=of 4] (5) {$t$} ; %
\path
  (0) edge [Dedge] node {2} (1)
  (0) edge [Dedge, bend left] node[right] {3} (3)
  (1) edge [Uedge] node {1} (2) 
  (1) edge [Uedge] node {1} (3) 
  (2) edge [Uedge] node {1} (4) 
  (3) edge [Uedge] node {1} (4) 
  (2) edge [Dedge, bend right] node[left] {1} (5)
  (4) edge [Dedge] node {3} (5)  ;

\end{tikzpicture}
\caption{S-T Network}
\label{fig:directed}
\end{figure}

We must now introduce some terms. First, to `cut' a network is to
remove edges from the network so that the network is split into two
unconnected, smaller networks. Second, an `s-t cut' is a cut on a s-t
network that causes the source node and target node to end up in
separate, unconnected networks. Finally, a `minimum cut' is an s-t cut
where the sum of cost associated with the removed edges is as small as
any other possible s-t cut. 

In our example, removing the edges $(z_2, t)$ and $(z_4, t)$ is an s-t
cut, but not the minimum cut. The minimum cut would be removing edges
$(z_1, z_2)$ and $(z_3, z_4)$ (Figure \ref{fig:mincut}).

\begin{figure}[!h]
\centering

\begin{tikzpicture}[auto, >= stealth', shorten >= 1pt, node
    distance=2cm, thick]
\tikzset{vertex/.style = {shape=circle, draw, minimum size = 1.5em}}
\tikzset{Dedge/.style = {->}}
\tikzset{Uedge/.style = {<->}}

\node[vertex, above=of 1] (0) {$s$} ; %
\node[vertex] (1) {$y_1$} ; %
\node[vertex, below left=of 1] (2) {$y_2$} ; %
\node[vertex, below right=of 1] (3) {$y_3$} ; %
\node[vertex, below left=of 3] (4) {$y_4$} ; %
\node[vertex, below=of 4] (5) {$t$} ; %
\path
  (0) edge [Dedge] node {2} (1)
  (0) edge [Dedge, bend left] node[right] {3} (3)
  (1) edge [Uedge] node {1} (3) 
  (2) edge [Uedge] node {1} (4) 
  (2) edge [Dedge, bend right] node[left] {1} (5)
  (4) edge [Dedge] node {3} (5)  ;

\end{tikzpicture}
\caption{Minimum Cut}
\label{fig:mincut}
\end{figure}

Every s-t cut can be mapped onto a pattern of neighborhood block
labels by letting $y_i = 1$ if node $z_i$ ends up in the $t$
subnetwork and $y_i = 0$ otherwise. Greig and his coauthors
demonstrated that the minimum s-t cut their constructed s-t network
always corresponded to the minimum scoring pattern of block
assignment.

Because computer scientists have known how to solve the minimum cut
problem swiftly since the 1950s, this meant that a large class of
network configurations problems were now tractable. Instead of
checking every possible pattern out of exponentially possibilities, we
can now find the best scoring pattern directly and
quickly.\cite{ford_maximal_1956}

\subsubsection*{Submodularity}
Greig's original result only applied to scoring functions 
where blocks could belong to only one of two classes and where the
scoring function had an important property called submodularity.  A
submodular scoring function would be one where $\epsilon_{i,j}(0,0) +
\epsilon_{i,j}(1,1) \leq \epsilon_{i,j}(0,1) + \epsilon_{i,j}(1,0)$
for all $i,j$ in $\mathcal{N}$. Our scoring function is decisively not
submodular because we want it to cost more to put together very
dissimilar neighboring blocks than to assign them to them to different
neighborhoods. That is, for very dissimilar blocks we want it to hold
that $\epsilon_{i,j}(0,0) + \epsilon_{i,j}(1,1) \boldsymbol{>}
\epsilon_{i,j}(0,1) + \epsilon_{i,j}(1,0)$.

Since we don't have a submodular scoring functions and often will have
more than two neighborhood names, we can't use the minimum cuts
algorithm directly to find the lowest scoring neighborhood
pattern. However, since the 1980s computers scientists have developed
other graph cutting methods that allow us to find approximate
solutions to multi-label non-submodular scoring problems like ours
\cite{something}.

\section*{Learning similarities}
In the previous section, we showed how we could construct a scoring
function so that a preferred pattern of block assignments would have
the best score. Now we will turn to how, given a preferred assignment,
we can learn such a scoring function. 

We will first make our lives easier by only considering simple, linear
similarity terms. As before, the scoring function will be

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i,j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j,\phi_{i,j})
\end{align}

\noindent
and

\begin{equation}
\epsilon_{i,j}(y_i,y_j,\phi_{i,j}) = \begin{cases}
  0 &y_i = y_j \\
  \phi_{i,j} &y_i \neq y_j
\end{cases}
\end{equation}

\noindent
but now, let the similarity term $\phi_{i,j}$ be the weighted sum of
observed similarities between block $i$ and $j$. 

\begin{align}
\phi_{i,j} = w_0 + w_1s_{1,i,j} + w_2s_{2,i,j} + ... + w_ns_{n,i,j}
\end{align} 

\noindent
An observed inter-block similarity could be absolute difference in
population, or dummy variable indicating whether the blocks are
separated by a railroad, or similar.

\noindent

With this form, learning a scoring function means finding weights
that give a better score to our preferred pattern of neighborhood
assignments than any other pattern. In other words, we want to find
some weights, $\mathbf{w}$, that solves this system of linear
equations:

\begin*{equations}
\operatorname{E}(\mathbf{y}_1, \mathbf{s}, \mathbf{w})
\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \\
\operatorname{E}(\mathbf{y}_2, \mathbf{s}, \mathbf{w})
\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w})
\ldots \\
\operatorname{E}(\mathbf{y}_{M-1}, \mathbf{s}, \mathbf{w})
\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w})
\operatorname{E}(\mathbf{y}_{M}, \mathbf{s}, \mathbf{w})
\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w})
\end{equations}

Where \mathbf{y}^* is our preferred pattern, and M is the number of
possible neighborhood patterns.

\subsubsection*{Specifying a unique solution}
In order to use some convenient machinery, we will modify the learning
problem somewhat. These modifications will only act to specify a
particular solution to the system of inequalities. Any solution to the
modified problems is also a solution to the system of inequalities and
if there is solution to the system of inequalities there will be a
solution to the modified problems.

First, observe that if a set of weights gives the preferred assignment
a lower score than any other assignment, there is some difference
between the score of the preferred assignment and the score of the
next best scoring assignment. Call this difference the 'margin. Our
first modification to the learning problem is that we now seek the
weights that give our preferred assignment the largest margin.

\begin{align}
&\argmax_{\mathbf{w}} \mathbf{\gamma} \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq \gamma\\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in the set of
  possible neighborhood assignments}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align}

This still does not specify a unique set of weights. To do that, we
constrain the sizes of the weights. We could do this by
limiting the sum of the absolute values of the weights. A similar, but
more mathematically convenient choice is to require that
$\sqrt{\sum_i^M w_i^2 = 1}$. 

%
\begin{align*}
&\argmax_{\mathbf{w}:\sqrt{\sum_i^M w_i^2 = 1}=1} \mathbf{\gamma} \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq \gamma\\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in the set of
  possible neighborhood assignments}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align*}
%

Here $\mathbf{y}*$ is the target neighborhood assignment and $\mathbf{s}$
are similarity measures between blocks. 

Large margin problems like this one have an equivalent canonical
representation: 
%
\begin{align*}
&\argmin_{\mathbf{w}} \frac{1}{2}||\mathbf{w}||^2 \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq 1 \\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in the set of
  possible neighborhood assignments}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align*}

This is quadratic program, a well known class of constrained
optimization problems. Unfortunately, we cannot solve this problem
directly because the number of constraints is typically too
large. That is, we require every possible pattern to have a
higher score than our target pattern, and the number of possible
patterns grows exponentially with the number of blocks.

Instead, we'll solve similar problem that, in practice, we can solve
quickly, and will also learn a set of weights that still will give our
target pattern lower energy than any other
pattern.\cite{szummer_learning_2008}
 

\subsection*{Learning Routine}
First, we initialize the weights to some starting value, create an
empty set of constraints $\mathcal{K}$, and set a counter $i$ to $0$.
\begin{enumerate}
\item Use a minimum cuts algorithm to find the neighborhood pattern
  that has a lower score than any pattern given the current weights.
  Call this pattern $\mathbf{y}_i$ and add it to the constraint
  set $\mathcal{K}$.

\item Update the weights by solving the quadratic program: 
%
\begin{align*}
&\argmin_{\mathbf{w}} \frac{1}{2}||\mathbf{w}||^2 \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq 1 \\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in } \mathcal{K}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align*}
%

\item If the weights changed in the previous step, set $i = i + 1$ and
  go to step 1. If the weights did not change, stop the routine.
\end{enumerate}

If we find a set of weights that gives our target pattern a lower
energy then any pattern in the constraint set $\mathcal{K}$, then we
are assured that our target pattern has a lower energy than any other
possible pattern. If another pattern had a lower score given the final
set of weights, then it would have been added to $\mathcal{K}$ in step
1, and our optimization routine would have continued. 

\subsection*{Empirical Loss}
Some readers may have noticed that the our original four-city block example
still does not have a unique solution for the learning problem we have
set up. There are two different neighborhood assignments that have the
same lowest score, and they both achieve our goal of putting similar
blocks in the same neighborhood and dissimilar blocks in different
neighborhoods (Table \ref{table:lowest}).

\begin{table}
\centering
  \begin{tabular}{cc}
      \tikz{ %
        \node[latent] (1) {$0$} ; %
        \node[latent, below left=of 1] (2) {$0$} ; %
        \node[latent, fill=black, below right=of 1] (3) {\textcolor{white}{$1$}} ; %
        \node[latent, fill=black, below left=of 3] (4) {\textcolor{white}{$1$}} ; %
        \factor[below left=of 1] {1-2} {} {} {} ;
        \factor[below right=of 1] {1-3} {} {} {} ;
        \factor[below right=of 2] {2-4} {} {} {} ;
        \factor[below left=of 3] {3-4} {} {} {} ;
        \factoredge[-] {1} {1-2} {2} ; %
        \factoredge[-] {1} {1-3} {3} ; %
        \factoredge[-] {2} {2-4} {4} ; %
        \factoredge[-] {3} {3-4} {4} ; %
      } 
    &
      \tikz{ %
        \node[latent, fill=black] (1) {\textcolor{white}{$1$}} ; %
        \node[latent, fill=black, below left=of 1] (2) {\textcolor{white}{$1$}} ; %
        \node[latent, below right=of 1] (3) {$0$} ; %
        \node[latent, below left=of 3] (4) {$0$} ; %
        \factor[below left=of 1] {1-2} {} {} {} ;
        \factor[below right=of 1] {1-3} {} {} {} ;
        \factor[below right=of 2] {2-4} {} {} {} ;
        \factor[below left=of 3] {3-4} {} {} {} ;
        \factoredge[-] {1} {1-2} {2} ; %
        \factoredge[-] {1} {1-3} {3} ; %
        \factoredge[-] {2} {2-4} {4} ; %
        \factoredge[-] {3} {3-4} {4} ; %
      } 
    \\
  \end{tabular}
  \caption{Equivalent Lowest Assignments}
  \label{table:lowest}
\end{table}

Happily, and for unrelated reasons, practicioners have found that good
performance for this type of learning problem depends upon incorporating an
empirical loss function into the objective function. If we follow
suit, we will also side step this problem of multiple equivalent
assignments. 

The learning routine is largely the same as above, but we change the
quadratic program in step 2:
%
\begin{align*}
&\argmin_{\mathbf{w}} \frac{1}{2}||\mathbf{w}||^2 +
  C\cdot\xi\\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq
\Delta(\mathbf{y}^*, \mathbf{y}) - \xi\\ 
& \xi \geq 0\\
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in the set of
  possible neighborhood assignments,}\\
&\xi \text{ is a slack variable, and } \Delta \text{ is an empirical loss.}\\
\end{align*}
%

The key to solving this problem is finding assignments that have the
lowest score, where the score incorporates the empirical loss: i.e.
%
\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j) + C\cdot\Delta(\mathbf{y}*,
\mathbf{y})
\end{align}
%
If the loss can be decomposed over the blocks like
%
\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j) + C\cdot\sum_i^N\Delta(y_i*, y_i)
\end{align}
%
then the objective has the form that allows us to use graph cutting
problems to find an exact or approximate minimum pattern. If the
loss function does not decompose we have to check an exponentially
many possible assignment, typically an intractable problem.

Ideally, we want loss function that increases the further we are from our
target assignment. A natural choice is to penalize every block that is
incorrectly assigned. If we penalize every error the same then we have
the classic Hamming loss. 

$\sum_i^N\begin{cases}
  0 &y_i^* = y_i \\
  1 &y_i^* \neq y_i
\end{cases}$. 

Incorporating this empirical loss into our learning problem means
that, of a set of previously equivalent assignments, only the
assignment closest to the training data will be have the lowest score.


For the same reason, traditional Markov Chain Monte Carlo
methods for finding the lowest scoring assignment also prove
intractable.\footnote{In order to converge on the mode of the
  distribution of scores, we have to calculate a normalizing constant,
  which is the sum of the scores of all possible assignments. This is
  computationally too expensive. There have been a number of attempts
  to find an acceptable substitute for the normalizing constant, but
  the empirical results have disappointed.\cite{li_mrf_2009}}
