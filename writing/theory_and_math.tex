\subsection{Neighborhood Model Representation}
Let's imagine a very small city consisting of four blocks. We can
represent this tiny town as a network where two blocks are connected
if they face the same street. In this representation, blocks that are
kitty corner are not directly connected. We'll index the blocks as
$1$, $2$, $3$, and $4$.

\begin{figure}[h]
\centering
\tikz{
\draw[help lines] (0,0) grid (2,2);
\node at (0.5, 0.5) {3} ;
\node at (1.5, 1.5) {2} ;
\node at (0.5, 1.5) {1} ;
\node at (1.5, 0.5) {4} ;
}
\end{figure}

\begin{figure}[h]
\centering

\tikz{ %
  \node[latent] (1) {$1$} ; %
  \node[latent, below left=of 1] (2) {$2$} ; %
  \node[latent, below right=of 1] (3) {$3$} ; %
  \node[latent, below left=of 3] (4) {$4$} ; %
  \edge[-] {2,3} {1} ; %
  \edge[-] {2,3} {4} ; %
}

\end{figure}

Suppose that, in our city, there are two neighborhoods. Each block
belongs to either one or the other of these neighborhoods. We want
similar blocks to belong to the same neighborhood. One way to
formalize this desire is to score every possible pattern of
neighborhood labels in such a way that our preferred patterns have the
best score.

First, we need some more precise terms. Let the two neighborhoods be
called $0$ and $1$. A block belongs to either neighborhood $0$ or
$1$, and we will denote this membership as $y_i$, so that $y_1=0$ is
equivalent to saying that block $1$ belongs to the $0$
neighborhood. Let the similarity between blocks $i$ and $j$ be called
$\phi_{i,j}$.

\begin{figure}[!h]
\centering

\tikz{ %
  \node[latent] (1) {$y_1$} ; %
  \node[latent, below left=of 1] (2) {$y_2$} ; %
  \node[latent, below right=of 1] (3) {$y_3$} ; %
  \node[latent, below left=of 3] (4) {$y_4$} ; %
  \factor[below left=of 1] {1-2} {$\phi_{1,2}$} {} {} ;
  \factor[below right=of 1] {1-3} {$\phi_{1,3}$} {} {} ;
  \factor[below right=of 2] {2-4} {$\phi_{2,4}$} {} {} ;
  \factor[below left=of 3] {3-4} {$\phi_{3,4}$} {} {} ;
  \factoredge[-] {1} {1-2} {2} ; %
  \factoredge[-] {1} {1-3} {3} ; %
  \factoredge[-] {2} {2-4} {4} ; %
  \factoredge[-] {3} {3-4} {4} ; %
  %\edge[-] {2,3} {4} ; %
}

\end{figure}


Let a particular pattern of assignment of blocks to neighborhoods
be called $\mathbf{y}$.  The score of $\mathbf{y}$ will be
$\operatorname{E}(\mathbf{y})$\footnote{$\operatorname{E}$ as in
  energy not expectation} which will take the following form:

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i,j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j,\phi_{i,j})
\end{align}

Where $<i,j>$ indexes a pair of neighboring blocks, and where $i < j$.
$\mathcal{N}$ are all these indices of neighboring blocks. The
$\epsilon_{i,j}$ function is 

\begin{equation}
\epsilon_{i,j}(y_i,y_j,\phi_{i,j}) = \begin{cases}
  0 &y_i = y_j \\
  \phi_{i,j} &y_i \neq y_j
\end{cases}
\end{equation}

Suppose that we want our preferred neighborhood assignment to have the
lowest score. If our similarity measures $\phi_{i,j}$ are positive
when blocks are similar and negative when blocks are different, then
we will encourage similar, neighboring blocks to belong to the same
neighborhood. 

Let our city have the following $\phi$'s.

\begin{align*}
&\phi_{1,2} = 1 \\
&\phi_{1,3} = -1 \\
&\phi_{2,4} = -1 \\
&\phi_{3,4} = 1
\end{align*} 

\noindent
If we now score every possible pattern of neighborhood labels, we will
find that the lowest scoring assignments are ones that put blocks $1$
and $2$ in one neighborhood and $3$ and $4$ in the other (Table
\ref{table:lowest}, Table \ref{table:energy}). By choosing the right
$\phi$'s we can have made our preferred pattern have the lowest scores.

\begin{table}
\centering
  \begin{tabular}{cc}
      \tikz{ %
        \node[latent] (1) {$y_1$} ; %
        \node[latent, below left=of 1] (2) {$y_2$} ; %
        \node[latent, fill=black, below right=of 1] (3) {\textcolor{white}{$y_3$}} ; %
        \node[latent, fill=black, below left=of 3] (4) {\textcolor{white}{$y_4$}} ; %
        \factor[below left=of 1] {1-2} {$1$} {} {} ;
        \factor[below right=of 1] {1-3} {$-1$} {} {} ;
        \factor[below right=of 2] {2-4} {$-1$} {} {} ;
        \factor[below left=of 3] {3-4} {$1$} {} {} ;
        \factoredge[-] {1} {1-2} {2} ; %
        \factoredge[-] {1} {1-3} {3} ; %
        \factoredge[-] {2} {2-4} {4} ; %
        \factoredge[-] {3} {3-4} {4} ; %
      } 
    &
      \tikz{ %
        \node[latent, fill=black] (1) {\textcolor{white}{$y_1$}} ; %
        \node[latent, fill=black, below left=of 1] (2) {\textcolor{white}{$y_2$}} ; %
        \node[latent, below right=of 1] (3) {$y_3$} ; %
        \node[latent, below left=of 3] (4) {$y_4$} ; %
        \factor[below left=of 1] {1-2} {$1$} {} {} ;
        \factor[below right=of 1] {1-3} {$-1$} {} {} ;
        \factor[below right=of 2] {2-4} {$-1$} {} {} ;
        \factor[below left=of 3] {3-4} {$1$} {} {} ;
        \factoredge[-] {1} {1-2} {2} ; %
        \factoredge[-] {1} {1-3} {3} ; %
        \factoredge[-] {2} {2-4} {4} ; %
        \factoredge[-] {3} {3-4} {4} ; %
      } 
    \\
  \end{tabular}
  \caption{Preferred Assignment: If $y_i = 0$, the block is colored
    white. If $y_i = 1$, the block is black.}
  \label{table:lowest}
\end{table}

\begin{table}[h]
\input{energy_table.tex}
\caption{Scores of Neighborhood Assignments}
\label{table:energy}
\end{table}

\subsection{Learning similarities}
Suppose we knew that we wanted some particular pattern of
neighborhoods, but we didn't know what similarity scores, $\phi$'s
would give that pattern the lowest score. Given a candidate set of
$\phi$'s, we could start by checking that these similarity scores gave
the preferred pattern the best score.

For small networks, we could check the scores for each individual
pattern, but this exhaustive strategy becomes quickly impractical
for larger cities. With two possible neighborhoods, the number of
possible assignments is $2^N$ where N is the number of blocks. This
permutational explosion means that for even small cities, we cannot
possibly check every possible neighborhood assignment in human-scale
time.

\subsubsection{Network Methods}
Surprisingly, there is a method for finding the lowest scoring
assignment given some $\phi$'s in polynomial time. Greig, Porteous,
and Sehult demonstrated that finding the lowest scoring assignment for
a two neighborhood case was equivalent to solving the problem of
finding the minimum cut of a network.\cite{greig_exact_1989}

Greig and his co-authors set up a more general scoring problem. In
addition to terms that depended upon pairs of neighbors, their score
also included terms for individual nodes.

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_i\epsilon_i(y_i) + \sum_{<i
  j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j,\phi_{i,j})
\end{align}

\noindent
The node terms were of one of two forms, either

\begin{equation}
\epsilon_{i}(y_i) = \begin{cases}
  0 &y_i = 0 \\
  \phi_{i} \geq 0 &y_i = 1
\end{cases}
\end{equation}

\noindent
or 

\begin{equation}
\epsilon_{i}(y_i) = \begin{cases}
  \phi_{i} \geq 0 &y_i = 0 \\
  0 &y_i = 1
\end{cases}
\end{equation}

\noindent
They had the same form of $\epsilon_{i,j}$ as we do

\begin{equation}
\epsilon_{i,j}(y_i,y_j,\phi_{i,j}) = \begin{cases}
  0 &y_i = y_j \\
  \phi_{i,j} &y_i \neq y_j
\end{cases}
\end{equation}

\noindent
but unlike us, $\phi_{i,j} \geq 0$ for all $<i, j>$ in $\mathcal{N}$. 

One of their networks could look like this

\begin{figure}[!h]
\centering

\tikz{ %
  \node[latent] (1) {$y_1$} ; %
  \node[latent, below left=of 1] (2) {$y_2$} ; %
  \node[latent, below right=of 1] (3) {$y_3$} ; %
  \node[latent, below left=of 3] (4) {$y_4$} ; %
  \factor[below left=of 1] {1-2} {$1$} {} {} ;
  \factor[below right=of 1] {1-3} {$1$} {} {} ;
  \factor[below right=of 2] {2-4} {$1$} {} {} ;
  \factor[below left=of 3] {3-4} {$1$} {} {} ;
  \factoredge[-] {1} {1-2} {2} ; %
  \factoredge[-] {1} {1-3} {3} ; %
  \factoredge[-] {2} {2-4} {4} ; %
  \factoredge[-] {3} {3-4} {4} ; %
  %\edge[-] {2,3} {4} ; %
}

\end{figure}

Based on this undirected network, Greig and his co-authors constructed
a special, directed network where every edge has a cost associated
with it. For every variable $y_i$ in the undirected network, the
directed network contains a node $z_i$.  For every edge $(y_i, y_j)$ in the
undirected network, they introduced a directed edge from $z_i$ to $z_j$ and
another from $z_j$ to $z_i$ with associated costs of $\phi_{i,j}$.

In addition to the original nodes, they also introduced two special
nodes: a source node called $s$ and a target node called
$t$. If $\epsilon_i(0) = 0$ then there would be a directed edge from
$s$ to the $i$th node. If $\epsilon_i(1) = 0$ then there would be
directed edge from the $i$th node to $t$. Networks like this are
called s-t networks.

Suppose that $\epsilon_1(1)=2$, $\epsilon_2(0)=1$, $\epsilon_3(1)=3$,
and $\epsilon_4(0)=3$. The corresponding directed network is shown in
Figure \ref{fig:directed}. 

\begin{figure}[!h]
\centering

\begin{tikzpicture}[auto, >= stealth', shorten >= 1pt, node
    distance=2cm, thick]
\tikzset{vertex/.style = {shape=circle, draw, minimum size = 1.5em}}
\tikzset{Dedge/.style = {->}}
\tikzset{Uedge/.style = {<->}}

\node[vertex, above=of 1] (0) {$s$} ; %
\node[vertex] (1) {$z_1$} ; %
\node[vertex, below left=of 1] (2) {$z_2$} ; %
\node[vertex, below right=of 1] (3) {$z_3$} ; %
\node[vertex, below left=of 3] (4) {$z_4$} ; %
\node[vertex, below=of 4] (5) {$t$} ; %
\path
  (0) edge [Dedge] node {2} (1)
  (0) edge [Dedge, bend left] node[right] {3} (3)
  (1) edge [Uedge] node {1} (2) 
  (1) edge [Uedge] node {1} (3) 
  (2) edge [Uedge] node {1} (4) 
  (3) edge [Uedge] node {1} (4) 
  (2) edge [Dedge, bend right] node[left] {1} (5)
  (4) edge [Dedge] node {3} (5)  ;

\end{tikzpicture}
\caption{S-T Network}
\label{fig:directed}
\end{figure}

We must now introduce some terms. First, to `cut' a network is to
remove edges from the network so that the network is split into two
disconnected, smaller networks. Second, an `s-t cut' is a cut on a s-t
network that causes the source node and target node to end up in
separate, disconnected networks. Finally, a `minimum cut' is an s-t cut
where the sum of cost associated with the removed edges is as small as
any other possible s-t cut. 

In our example, removing the edges $(z_2, t)$ and $(z_4, t)$ is an s-t
cut, but not the minimum cut. The minimum cut would be removing edges
$(z_1, z_2)$ and $(z_3, z_4)$ (Figure \ref{fig:mincut}).

\begin{figure}[!h]
\centering

\begin{tikzpicture}[auto, >= stealth', shorten >= 1pt, node
    distance=2cm, thick]
\tikzset{vertex/.style = {shape=circle, draw, minimum size = 1.5em}}
\tikzset{Dedge/.style = {->}}
\tikzset{Uedge/.style = {<->}}

\node[vertex, above=of 1] (0) {$s$} ; %
\node[vertex] (1) {$z_1$} ; %
\node[vertex, below left=of 1] (2) {$z_2$} ; %
\node[vertex, below right=of 1] (3) {$z_3$} ; %
\node[vertex, below left=of 3] (4) {$z_4$} ; %
\node[vertex, below=of 4] (5) {$t$} ; %
\path
  (0) edge [Dedge] node {2} (1)
  (0) edge [Dedge, bend left] node[right] {3} (3)
  (1) edge [Uedge] node {1} (3) 
  (2) edge [Uedge] node {1} (4) 
  (2) edge [Dedge, bend right] node[left] {1} (5)
  (4) edge [Dedge] node {3} (5)  ;

\end{tikzpicture}
\caption{Minimum Cut}
\label{fig:mincut}
\end{figure}

Every s-t cut can be mapped onto a pattern of neighborhood block
labels by letting $y_i = 1$ if node $z_i$ ends up in the $t$
subnetwork and $y_i = 0$ otherwise. Greig and his coauthors
demonstrated that the minimum cut of their constructed s-t network
always corresponded to the minimum scoring pattern of block
assignment.

Because computer scientists have known how to solve the minimum cut
problem swiftly since the 1950s, this meant that a large class of
network configurations problems were now tractable. Instead of
checking every possible pattern out of exponentially possibilities, we
can now find the best scoring pattern directly and
quickly.\cite{ford_maximal_1956}

In particular, if we want to see if a certain set of similarities,
$\phi's$ gives a preferred pattern of neighborhood labels a lower
score than any other pattern, we can apply Greig's approach. If the
minimum cut of the associated s-t network corresponds to our preferred
pattern, then we know our $\phi$'s gives us our
preferred pattern the lowest score. If the minimum cut corresponds to
some other pattern, then we know that these $\phi$'s do not give our
preferred pattern the lowest score.

\paragraph{Submodularity}
Greig and his coauthors' original result only applied to scoring functions 
where blocks could belong to only one of two classes and where the
scoring function had an important property called submodularity.  A
submodular scoring function would be one where $\epsilon_{i,j}(0,0) +
\epsilon_{i,j}(1,1) \leq \epsilon_{i,j}(0,1) + \epsilon_{i,j}(1,0)$
for all $i,j$ in $\mathcal{N}$. Our scoring function is decisively not
submodular because we want it to cost more to put together very
dissimilar neighboring blocks than to assign them to different
neighborhoods. That is, for very dissimilar blocks we want it to hold
that $\epsilon_{i,j}(0,0) + \epsilon_{i,j}(1,1) \boldsymbol{>}
\epsilon_{i,j}(0,1) + \epsilon_{i,j}(1,0)$.

Since we don't have a submodular scoring function and will often have
more than two neighborhood names, we can't use the minimum cuts
algorithm directly to find the lowest scoring neighborhood
pattern. Since the 1980s computers scientists have developed
other graph cutting methods that allow us to find approximate
solutions to multi-label non-submodular scoring problems like ours
\cite{kolmogorov_minimizing_2007}

\subsection{Learning similarities}
These network methods mean that we can quickly check if a
particular set of inter-block similarities gives a preferred
neighborhood pattern the best score. We will now describe how to use
this to efficiently find inter-block similarities that will give a
preferred pattern the lowest score.

We will make our lives easier by only considering simple, linear
similarity terms. As before, the scoring function will be

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i,j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j,\phi_{i,j})
\end{align}

\noindent
and

\begin{equation}
\epsilon_{i,j}(y_i,y_j,\phi_{i,j}) = \begin{cases}
  0 &y_i = y_j \\
  \phi_{i,j} &y_i \neq y_j
\end{cases}
\end{equation}

\noindent
but now, let the similarity term $\phi_{i,j}$ be the weighted sum of
observed similarities between block $i$ and $j$. 

\begin{align}
\phi_{i,j} = w_0 + w_1s_{1,i,j} + w_2s_{2,i,j} + ... + w_ns_{n,i,j}
\end{align} 

\noindent
An observed inter-block similarity could be absolute difference in
population, or dummy variable indicating whether the blocks are
separated by a highway, railroad embankment, or similar.

\noindent

With this form, learning a scoring function means finding weights
that give a better score to our preferred pattern of neighborhood
assignments than any other pattern. In other words, we want to find
some weights, $\mathbf{w}$, that solve this system of linear
equations:

\begin{align*}
\operatorname{E}(\mathbf{y}_1, \mathbf{s}, \mathbf{w})
&\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \\
\operatorname{E}(\mathbf{y}_2, \mathbf{s}, \mathbf{w})
&\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \\
&\ldots \\
\operatorname{E}(\mathbf{y}_{M-1}, \mathbf{s}, \mathbf{w})
&\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \\
\operatorname{E}(\mathbf{y}_{M}, \mathbf{s}, \mathbf{w})
&\geq \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w})
\end{align*}

Where $\mathbf{y}^*$ is our preferred pattern, M is the number of
possible neighborhood patterns, and $\mathbf{s}$ are the similarities
between blocks.

\subsubsection{Specifying a unique solution}
In order to use some convenient machinery, we will modify the learning
problem somewhat. These modifications will only act to specify a
particular solution to the system of inequalities. Any solution to the
modified problems is also a solution to the system of inequalities and
if there is a solution to the system of inequalities there will be a
solution to the modified problems.

First, observe that if a set of weights gives the preferred assignment
a lower score than any other assignment, there is some difference
between the score of the preferred assignment and the score of the
next best scoring assignment. Call this difference the `margin.' Our
first modification to the learning problem is that we now seek the
weights that give our preferred assignment the largest margin.

\begin{align}
&\argmax_{\mathbf{w}} \mathbf{\gamma} \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq \gamma\\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in the set of
  possible neighborhood assignments}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align}

This still does not specify a unique set of weights. To do that, we
constrain the sizes of the weights. We could do this in many ways. For
example, we could limit the sum of the absolute values of the weights. As
will be momentarily clear, a more convenient choice is to require that
$\sqrt{\sum_i^M w_i^2} = 1$. The learning problem is now:

%
\begin{align*}
&\argmax_{\mathbf{w}:\sqrt{\sum_i^M w_i^2}=1} \mathbf{\gamma} \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq \gamma\\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in the set of
  possible neighborhood assignments}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align*}
%

With these two modifications, the learning problem is now a quadratic
program, a well known class of constrained optimization
problems.\footnote{The problem is easier to recognize as a quadratic
  program in its dual, canonical form:
%
\begin{align*}
&\argmin_{\mathbf{w}} \frac{1}{2}||\mathbf{w}||^2 \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq 1 \\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in the set of
  possible neighborhood assignments}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align*}
}

Unfortunately, we cannot directly use an off-the-shelf quadratic
program solver, because the number of constraints will typically be
too large. We require every possible pattern to have a higher
score than our target pattern, and the number of possible patterns
grows exponentially with the number of blocks.

Instead, we will solve a similar problem that takes advantage of the
network cutting algorithm and so can be solved
quickly. The algorithm for solving this problem is typically called
the `structured support vector machine'\cite{szummer_learning_2008}.
 

\subsubsection{Structured Support Vector Machine}
First, we initialize the weights to some starting value, create an
empty set of constraints $\mathcal{K}$, and set a counter $i$ to $0$.
\begin{enumerate}
\item Use a minimum cuts algorithm to find the neighborhood pattern
  that has a lower score than any pattern given the current weights.
  Call this pattern $\mathbf{y}_i$ and add it to the constraint
  set $\mathcal{K}$.

\item Update the weights by solving the quadratic program: 
%
\begin{align*}
&\argmax_{\mathbf{w}:\sqrt{\sum_i^M w_i^2}=1} \mathbf{\gamma} \\
&\text{such that} \\
&\operatorname{E}(\mathbf{y}, \mathbf{s}, \mathbf{w})
- \operatorname{E}(\mathbf{y}^*, \mathbf{s}, \mathbf{w}) \geq \gamma\\ 
&\text{for all } \mathbf{y} \text{ where } \mathbf{y} \text{ is in } \mathcal{K}\\
&\text{and } \mathbf{y} \neq \mathbf{y}^*
\end{align*}
%

\item If the weights changed in the previous step, set $i = i + 1$ and
  go to step 1. If the weights did not change, stop the routine.
\end{enumerate}

If we find a set of weights that gives our target pattern a lower
energy then any pattern in the constraint set $\mathcal{K}$, then we
are assured that our target pattern has a lower score than any other
possible pattern. If another pattern had a lower score given the final
set of weights, then it would have been added to $\mathcal{K}$ in step
1, and our optimization routine would have continued. 

With this structured support vector machine, we have the means of
efficiently learning weights that give a targeted pattern a better
score than any other pattern, but only if those weights exist. 

There is a similar, but more general form that can find weights that
either give the target pattern the lowest score or minimize the
difference between the score of the target pattern and another,
lowest scoring pattern. There is also a tweak that penalizes the
scores of patterns that empirically diverge from the target pattern,
and which can help the algorithm learn better weights for predicting
new patterns.

The details of these extensions are best found in x, y or z. For our
current purposes, we use one the extensions, where the slack variables
are weighted by block-by-block Hamming loss.
