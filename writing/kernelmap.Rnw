\documentclass{article}
\usepackage{amsmath}
\usepackage{url}
\usepackage{endnotes}
\usepackage{tikz}
\usepackage{subfigure}
\pgfrealjobname{modeling}
\usepackage[nogin]{Sweave}
\SweaveOpts{echo=F, width=4.5, height=4.75, prefix.string=/home/fgregg/sweave-cache/figs/fig, cache=T, results=hide}
<<preamble, cache=FALSE>>=
setwd('/home/fgregg/academic/boundaries/writing/')
setCacheDir('/home/fgregg/sweave-cache/values')
#pdf.options(maxRaster=256)
options(SweaveHooks=list(fig=function()
          par(mar=c(1.1, 1.1, 1.1, 1.1))))
@ 

\begin{document}
\section{Background}
Sociology is a discipline of latent variables. We never directly
observe a class, a political party, an organization, but only behavior
or traces of behavior that we believe to be expression of a hidden
principle. In this sociology is no different than any empirical
science. But unlike some sciences, sociology has had a hard time
finding ways to measure our most important latent variables so
that we believe these measurements to be reliable and valid.

While most sciences can be mainly concerned with accurately and
reliably measuring attributes of things, in the social sciences, we
must also put together the things that we want to measure like
markets, neighborhoods, or occupations. At our most innocent, we
face the danger of the fallacy of composition, and more commonly and
devastatingly we unintentionally gerrymander the world into the forms
we expected to see. While we have compiled wisdom about avoiding the
most common ways of fooling ourselves, we have little positive art on
how to collect observations together into reliable and valid sets.

Since, the development of Item Response Theory in the 1950s, we have
known how to combine many noisy, imprecise measurements in a
principled way to produce a accurate, precise estimates of a latent
trait of a unit. Strangely, we have never adapted these techniques for
the creation of units. For example, we might believe that an ethnic
group can organize friendships, marriages, consumption patterns,
political behavior, names, and self-understanding. But when we study
ethnic groups as things, we typically only use one, maybe two
criterion, which we do not believe to be all that accurate, to sort
people into groups with hard boundaries even though we think that
these groups could overlap and even nest.

We can do better. In this work, I will demonstrate how we can create
valid and reliable definitions of neighborhoods in a host of American
cities, by combining a variety observations that we believe to be
organized by neighborhoods: labeling by residents, foreclosure rates,
crime rates, and census data. Each of these types of data, when used
singly, would produce implausible neighborhood definitions, but in
combination should gain confidence that we are capturing something
beyond random variation and biases in the data. However, while this
work will produce neighborhood definitions that should be of the
greatest interest to urban sociologists, the method is applicable to
creating defensible definitions of many sociological units.











constructing measurements of hidden traits of units and means of
evaluating the reliability of the measure, and a host of means of
convincing ourselves of the validity. Yet, we have not adapted the
basic insight that differential outcomes of multiple measurements can
be combined in a principled way to produce precise measurements of a
hidden variable, even if the









Unlike most sciences, our fundamental methodological problem is not
the measurement of attributes of entities, but the measurement of
existence of those entities. Of course, we are also interested
attributes, but before

Unlike that other great discipline of latent variables, pyschometrics,
our recurrent problem is not the measurement of hidden attribute of a
element, but figuring out what elements make up hidden things and even
how thingy that always hidden thing is.  We have a body of literature
on how to do some of this that we call sampling design, or case
selection, or colligation. However, while this literature contains
much cautions against various ways of gerrymandering the world, we
have little art for positively evaluating whether the way that we
carve up the world makes sense.


Ultimately, we 





in the first
place. can plausibly claim that our collection is an instance of
something: a city, an industrial field, a country. This is hard.



ng problem we face
our classic variables are not hidden attributes of units, but the

I believe that this has not been lack of ability, but to lack of data,
and that in this moment of unparalleled and creepy recording of traces
of human behavior, we can do better. The main challenge we face, the
one we have always faced is how to convince ourselves about something
that we cannot see.

This work on neighborhood boundaries hopes to be evidence for that
hope. First, I will learn a set of neighborhood boundaries for the 20
largest American cities using a collection of labels that people have
applied to locations and existing theory of neighborhoods. Second, I
will evaluate whether the derived neighborhood boundaries correlates
with differences in demographics, as we would predict. Finally, I will
evaluate whether incorporating information about derived neighborhood
boundaries improves our predictions about spatial, social processes,
particularly crime rates.

This particular paper is about work on the first step, learning
neighborhood boundaries. 

\section{Data}
I have a nightly updated database of geocoded Craigslist apartment
rentals, sublet, and roommate listings. For most of these listings,
the poster entered some text in the ``Specific Location'' field. From
this data, we want to create a map of the probabilities that some
point in a city will be labeled by a neighborhood name.

\subsection{Pre-processing}
The first task must be to filter and normalize the ``Specific
Location'' labels into canonical neighborhood names. Aside from
variations of spelling of neighborhood names, some labels consist of
address or intersection information or claims that the listing is near
a train station, park, beach, or other generic landmark. We want to
discard such labels, and keep only labels that refer to a neighborhood
or some unique landmark, such as a named park or university.

Ideally, we would like to learn this filter and normalization mapping,
perhaps by creating a metric that combines a Levenshtein and Euclidean
distance metric. Alternatively, if we had sufficient data, we would
likely do well just to discard all labels that appear less than some
threshold. However for the case of Chicago, I have built up a set of
handcoded rules that works well.

<<getData, include=FALSE>>=
library(RMySQL)

con <- dbConnect(MySQL(), dbname="neighborhood")
listings <- dbGetQuery(con, "
SELECT DISTINCT
X(lat_long) AS y,
Y(lat_long) AS x,
label.label AS neighborhood
FROM location, label
WHERE location.location_id = label.location_id"
                       ) 
dbDisconnect(con)
@

<<normalizeNeighborhoods>>=
source("../code/normalizeNeighborhoods.R")
listings$neighborhood <- cleanLabels(listings$neighborhood)
@ 


For some listings, the poster enters more than one neighborhood in the
``Specific Location'' field, i.e. ``Lakeview/Lincoln Park''. For such
listings, we create a new location for each neighborhood in the label,
so, for the example above, it is as if we saw two listings at the same
location, one labeled ``Lakeview'' and the other ``Lincoln Park.''
Some other scheme of credit assignment might be worth exploring.

After all this processing, we make sure that every location-label pair
is unique. Almost all duplicates are due to the same poster reposting
the same listing, and such reposts contain no more information than
the first post. We throw away some information here, and we would be a
better off if we could only discard postings that we know are really
from the same poster.

<<splitAndUniquify>>=
listings <- splitNeighborhoods(listings)
listings <- unique(listings)
@ 

Finally, we reproject the latitude and longitude encoded coordinates to a
State Plane Coordinate System, particularly NAD83 / UTM zone 16N. We
will be using other geocoded inormation, and this is a good common
projection for Chicago.

<<GIS>>=
library(maptools)
library(rgdal)

listings <- SpatialPointsDataFrame(coords = listings[, c("x", "y")],
                                   data = data.frame(listings$neighborhood),
                                   proj4string = CRS("+proj=longlat")
                                   )
names(listings) <- c("neighborhood")
@ 


 

<<>>=
# projection = "+proj=utm +zone=16 +datum=NAD83"
projection = "+proj=tmerc +lat_0=36.66666666666666 +lon_0=-88.33333333333333 +k=0.9999749999999999 +x_0=300000 +y_0=0 +datum=NAD83 +units=us-ft +no_defs +ellps=GRS80 +towgs84=0,0,0"
listings <- spTransform(listings, CRS(projection))

@ 

\section{Density Estimation}
Now, we want to make a kernel density estimate for each
neighborhood. For now, we will only estimate the density of a subset
of neighborhoods on the near north side where Craigslist postings
are concentrated.

As our kernel density estimates will depend upon the variance of the
data, we attempt to find and remove outliers using the Hampel
test.\footnote{Lauire Davies and Ursula Gather. ``The Identification
  of Multiple Outliers.'' \emph{Journal of the American Statistical
    Association}, 1993, (88:423), p.782.}


\subsection{Bandwidth Selection}



<<trimOutliers>>=
normalize <- function(x) {
    (x - median(x))/mad(x, constant=1)
  }
  
hampelCriticalValue <- function(n, alpha, num.samples=100) {
  samples = matrix(rnorm(n * num.samples, 0, 1), n, num.samples)
  samples = normalize(samples)/0.6745
  maxValues = apply(samples,
                    MARGIN=2,
                    FUN=function(x) {max(abs(x))}
                    )
  quantile(maxValues, 1-alpha)
}


hampelOutliers <- function(points, alpha) {
  threshold <- hampelCriticalValue(dim(points)[1], alpha)

  points[abs(normalize(points@coords[,1])) < threshold &
         abs(normalize(points@coords[,2])) < threshold, ]
}

@ 
\begin{figure}[h!]
  \centering
  \subfigure[]{
<<fig=TRUE, tizk=FALSE, width=2, height=2.5>>=
plot(listings[listings$neighborhood=="lakeview",], pch='.')
box()
@ 
}
\subfigure[]{
<<fig=TRUE, tizk=FALSE, width=2, height=2.5>>=
plot(hampelOutliers(listings[listings$neighborhood=="lakeview",], .90), pch='.')
box()
@ 
}
\caption{(a) All ``Lakeview'' locations; (b)``Lakeview'' locations with outliers removed, CV = 0.9}
\end{figure}

The most important choice in kernel density estimation is selecting
the bandwidth, and I examined a number of multivariate bandwidth
selectors: drop in plugins, smoothed cross validation, cross validated
nearest neighbors, and cross validated adaptive nearest neighbors.

The most plausible looking results came from a drop-in plugin from the
`ks'
library\footnote{\url{http://cran.r-project.org/web/packages/ks/}}. This
method depends upon assuming, at one point, that the true density is
Gaussian. I would like to implement the bandwidth selection procedure
described by Botov et.al., which makes no such assumption and which
seems to make more efficient estimates for distributions which are not
Gaussian.\footnote{Botev, Grotowski, Kroese, ``Kernel Density
  Estimation Via Diffiusion.''  \emph{The Annals of Statistics} 2010,
  (38:5), p. 2916-2957.}

\section{Evaluation and Discussion}
After estimating the density for every neighborhood, we can plot them,
and the decision boundaries between them. In addition to calculating
the boundaries between neighborhoods, I define a threshold value, such
that if the probability that a point belongs to any neighborhood is
below that threshold I assign that point to a `no neighborhood' class.

In order to get a sense of plausibility of those boundaries we will
also plot the official boundaries of the Chicago Community Areas, the
river, and large city parks. We should expect that the geographic
features should make up many of the boundaries. At the median values
of neighborhood labels, I plot the neighborhood name and the number of
points that the density was estimated with.

<<prY>>=
freqHood <- function(hood, range, num.bins) {
  bins <- expand.grid(seq(range[1,1], range[2,1], length.out=num.bins),
                      seq(range[1,2], range[2,2], length.out=num.bins)
                      )
  coordinates(bins) <- ~Var1+Var2
  gridded(bins) <- TRUE
  proj4string(bins) <- proj4string(listings)
  
  o <- overlay(bins, hood)
  o <- as.data.frame(table(o))
  names(o) <- c("index", "count")
  o$index <- as.numeric(as.character(o$index))
  
  freq.hood <- rep(0, num.bins*num.bins)
  freq.hood[o$index] <- o$count
  freq.hood
  }
@ 

<<trainKDE>>=
trainKDE <- function(listings,
                     hoods,
                     range,
                     n.points,
                     kde,
                     no.hood.prior) {
  
  class.matrices <- array(, c(n.points, 
                              n.points, 
                              length(hoods)+1
                              )
                          )
  resolution = 10
  freq.hood <- matrix(0, resolution*resolution, length(hoods))
  
  total.listings = 0
  
  for (i in 1:length(hoods)) {
    hood.listings <- listings[listings@data$neighborhood == hoods[i],]
    hood.listings <- hampelOutliers(hood.listings, .90)
#    hood.listings$x <- jitter(hood.listings$x, amount=.001)
#    hood.listings$y <- jitter(hood.listings$y, amount=.001)

    num.listings <- dim(hood.listings)[1]
    
    p.density <- kde(hood.listings@coords, n.points, range)
    class.matrices[,,i] <- p.density * num.listings
    total.listings = total.listings + num.listings
  }
  
  class.matrices <- class.matrices/total.listings
  class.matrices[,,length(hoods)+1] <- matrix(no.hood.prior, 
                                              n.points, 
                                              n.points)
  class.matrices
}
@ 

<<trainModel>>=
neighborhoods <- c("lakeview","lincoln park", "roscoe village", 
                   "southport corridor", "wrigleyville", 
                   "boystown", "uptown", "buena park", 
                   "north center", "ravenswood", 
                   "lincoln square", "avondale", "old town",
                   "logan square", "bucktown", "wicker park",
                   "humboldt park", "andersonville", "albany park",
                   "irving park", "edgewater", "gold coast", 
"river north", "ukrainian village", "east village", "noble square", 
"streeterville", "west town", "magnificent mile", "river west")

#neighborhoods = c("lakeview", "lincoln park")

colors = rep(c('#B2DF8A', "#A6CEE3", "#1F78B4", "#33A02C", "#FB9A99",
  "#E31A1C", "#FDBF6F"),
             length.out = length(neighborhoods))

range = cbind(c(-87.7, -87.6), c(41.89,41.98))
range = project(range, projection)

n.points = 2^9
@ 
<<plotPredictions>>=
plotPredictions <- function(class.matrices,colors,k,range,boundaries=TRUE) {
  # Takes a X*Y*Number Array of Classes
  #
  n.points <- dim(class.matrices)[1]
  x = seq(range[1,1], range[2,1], length.out=n.points)
  y = seq(range[1,2], range[2,2], length.out=n.points)
  
  max.probs <- apply(class.matrices, 
                     MARGIN=c(1,2),
                     FUN=function(x) {max(x, na.rm=TRUE)})

  imageDensity <- function(x, y, z, color) {
    image(x,
          y,
          z, 
          col=rgb(t(col2rgb(color))/255,
                  alpha=seq(0,1,.1)),
          add=TRUE,
          breaks=c(seq(0,1,.1)/k, 1.1),
          useRaster=TRUE
          )

  }
  decisionBoundary <- function(x,y, z, max.probs) {
    contour(x,
            y,
            z - max.probs, 
            levels=c(0), 
            drawlabels=FALSE,
            add=TRUE,
            lty=1,
            lwd=.3,
            col="slategray")
  }

  for (i in 1:(dim(class.matrices)[3]-1)) {
    class.matrix <- class.matrices[,,i]
    imageDensity(x,y,class.matrix, colors[i])
    if (boundaries) {
      decisionBoundary(x,y,class.matrix, max.probs)
    }
    
  }
}
@           


<<KS>>=
library(ks)
ks.pi <- function(points, n.points, range) {
  H <- Hpi.diag(x=points)
  fhat <- kde(x=points, 
              H=H, 
              gridsize=c(n.points, n.points),
              xmin = range[1,],
              xmax = range[2,]
              )
  fhat$estimate/(sum(colSums(fhat$estimate)))
}
@ 


<<>>=
importSHPfile <- function(shapefile, layer.name, projection, ...) {
  spdf <- readOGR(shapefile, layer=layer.name, ...)
  spdf <- spTransform(spdf, CRS(projection))
  return(spdf)
}                      



com.areas <- importSHPfile("../admin_areas/chicomm.shp",
                           "chicomm",
                           projection,
                           p4s="+proj=longlat")

railroads <- importSHPfile("../barriers/Railroads.shp",
                           "Railroads",
                           projection)

water <- importSHPfile("../barriers/Kmlchicagowaterfeatures.kml",
                       "WATER_FEATURES",
                       projection)

parks <- importSHPfile("../barriers/Kmlchicagoparks.kml",
                       "Chicago Parks",
                       projection)

@ 


<<>>=
simplePlot <- function(range) {
  plot(range[,1], range[,2],
       type="n",
#       asp=1,
       axes=FALSE,
       yaxs="i",
       xaxs="i",
       xlab="",
       ylab="")
  plot(com.areas, border="grey", add=TRUE, lwd=.1)
  plot(water, col="#C0E7F3", border=0, add=TRUE)
  plot(parks[sapply(slot(parks, "polygons"), slot, "area") > 100000,],
       col="#DBEADC", border=0, add=TRUE)
  lines(railroads, lty=2, col="tan")
}
@

<<>>=
hood.medoids <- aggregate(listings@coords, 
                          by=list(listings$neighborhood), 
                          median)
names(hood.medoids) <- c("neighborhood", "x", "y")

hood.n <- aggregate(listings$neighborhood ,
                    by=list(listings$neighborhood), 
                    length)
names(hood.n) <- c("neighborhood", "n")

hood.medoids <- merge(hood.medoids, hood.n)
@ 
<<KStrain>>=
C = trainKDE(listings, neighborhoods, range, 
             n.points, ks.pi, 0.0000012)
@ 

\begin{figure}[h!]
  \centering
<<KSplot, fig=TRUE, pdf=TRUE, tikz=FALSE, eval=TRUE, height=4.75>>=
simplePlot(range)
plotPredictions(C, colors, 100000, range)

neighborhoods <- neighborhoods[neighborhoods != "southport corridor"]
neighborhood.medoids <- hood.medoids[match(neighborhoods, 
                                           hood.medoids$neighborhood),]
                                  
text(neighborhood.medoids[, c('x', 'y')],
     labels = paste(neighborhood.medoids$neighborhood,
                    neighborhood.medoids$n, sep=", "),
     col= "#00000063",
     cex=.3)
text(hood.medoids[hood.medoids$neighborhood == "southport corridor", 
                  c('x', 'y')],
     labels = paste("southport corridor",
       hood.medoids[hood.medoids$neighborhood == "southport corridor", "n"], sep=", "),
     col= "#00000063",
     cex=.3,
     srt=90)

@
\caption{Density and decision boundaries for North Side neighborhoods}
\end{figure}

The resulting decision boundaries follow our expected borders
remarkably well, particularly where we have many observations. The
decision boundary between Lakeview and Lincoln Park follows
Diversey. Roscoe Village and North Center are separated from Lakeview
by the railroad tracks. Humboldt Park and Wicker Park are divided from
Logan Square and Bucktown by North Avenue. Almost no neighborhood
crosses the river, and where a decision boundary does cross the water
it is for neighborhoods where we have relatively little support.

Interestingly, both Ravenswood and Andersonville lay across the
tracks, and this does not seem to be an approximation error.

It seems like we could definitely benefit from incorporating beliefs
that neighborhood boundaries fall along certain kinds of geographic
features, but we will need to allow neighborhoods to cross these lines
if the data strongly suggest that they do.




<<KSplot1, fig=TRUE, pdf=TRUE, tikz=FALSE, eval=FALSE, height=4.75>>=
simplePlot(range)
plotPredictions(C, colors, 100000, range, boundaries=FALSE)

neighborhoods <- neighborhoods[neighborhoods != "southport corridor"]
neighborhood.medoids <- hood.medoids[match(neighborhoods, 
                                           hood.medoids$neighborhood),]
                                  
text(neighborhood.medoids[, c('x', 'y')],
     labels = paste(neighborhood.medoids$neighborhood,
                    neighborhood.medoids$n, sep=", "),
     col= "#00000063",
     cex=.3)
text(hood.medoids[hood.medoids$neighborhood == "southport corridor", 
                  c('x', 'y')],
     labels = paste("southport corridor",
       hood.medoids[hood.medoids$neighborhood == "southport corridor", "n"], sep=", "),
     col= "#00000063",
     cex=.3,
     srt=90)

@

<<allChicagoTrain, eval=FALSE>>=
range <-cbind(c(429300.8, 457830.8), c(4605821, 4655485))
neighborhoods = hood.medoids[hood.medoids$n > 5, 'neighborhood']
neighborhoods = neighborhoods[neighborhoods != ""]
C.all = trainKDE(listings, neighborhoods, range, n.points, ks.pi, 0.0000012)
simplePlot(range)
@ 

\begin{figure}
  \centering
<<fig=TRUE, pdf=TRUE, tikz=FALSE, height=7.833, eval=FALSE>>=
simplePlot(range)
colors = rep(c('#B2DF8A', "#A6CEE3", "#1F78B4", "#33A02C", "#FB9A99",
  "#E31A1C", "#FDBF6F"),
             length.out = length(neighborhoods)+1)
colors = sample(colors)
plotPredictions(C.all, colors, 20000, range, boundaries=FALSE)

neighborhoods <- neighborhoods[neighborhoods != "southport corridor"]
neighborhood.medoids <- hood.medoids[match(neighborhoods, 
                                           hood.medoids$neighborhood),]
                                  
text(neighborhood.medoids[, c('x', 'y')],
     labels = paste(neighborhood.medoids$neighborhood,
                    neighborhood.medoids$n, sep=", "),
     col= "#00000063",
     cex=.2)
text(hood.medoids[hood.medoids$neighborhood == "southport corridor", 
                  c('x', 'y')],
     labels = paste("southport corridor",
       hood.medoids[hood.medoids$neighborhood == "southport corridor", "n"], sep=", "),
     col= "#00000063",
     cex=.2,
     srt=90)
@ 
\caption{All Chicago Neighborhoods with more than 5 locations}
\end{figure}

\end{document}

We mainly have two strategies: obviousness and theoretic, but simple
classifiers. For some questions, there seems to be only one reasonable
grouping and identifying members of that grouping is simple.  If we
want to measure teacher ability, then classrooms are a natural
grouping and there is little difficulty in saying which students and
teachers belong to a particular class. While we may believe that much
happens outside the classroom that will affect student performance, we
accept that if the teacher is affecting the students achievement, she
is affecting him in the classroom and not within a different grouping.

For most of the groupings, though, we have no such clear and simple
way of deciding who belongs to what.  Sometimes this is because
membership seems to be graduated, sometimes because the grouping
itself is hard to define, and often both. We know of no well accepted
procedure for identifying the members of a class, city, political
parties, industrial fields, metropolitan areas, social networks. The
analyst must define some criterion that she hopes will do an adequate
job for her specific research question. Insofar as she is explicit,
the criterion is almost always very simple and in quantitative research,
it's usually a threshold on a handful of variables.\footnote{For
  example, the extent of the Census's metropolitan statistical areas
  is defined by the counties where 25\% of the employed workers
  commute to a central metropolitan county or 25\% of the employees in
  a county commute from that metro county.}






Our core concepts are  It is the critical the about identifying valid latent
classes and finding relations among them. 



When attempting to understand our world, the first problem an
empirical social scientists faces is to identify how a set of
observations is \emph{about} something. Because of the finiteness of
time, money, and patience, we can only observe and analyze a limited
set of behaviors and traces of behavior, and we have to convince
ourselves and our colleagues that we chose a meaningful set.
 
Studying individuals, psychometrics has built up a techniques for
evaluating the reliability and validity of observations. If we believe
that we have identified some meaningful grouping of individuals, we
can extend these practices to make measures of a group properties,
what Raudenbush calls ecometrics. However, we have very little of
either theory or accepted art about how we should identify
meaningful groupings.

Say we have some simple procedure to identify members of a
grouping. As an example let take Census's procedure for identifying
Metropolitan Statistical Areas (MSA): a county belongs to a MSA if 25\% of
the employed workers commute to a central metropolitan county or 25\%
of the employees in a county commute from that metro county. How would
we evaluate if this a good procedure?

First, we have to 
If we conceive of an MSA, or any classification we make, as an
approximation of a true grouping that we cannot directly observe, then
we have two ways forward. First, 


Borrowing from psychometrics, if we get the
same groupings by using different, independent features, then we may
become more confident that the grouping is not just an artifact of a
particular criterion.\footnote{Ideally, we should want features that
  are independent conditional on the true hidden class.}

For many of the groupings we are interested in, 





One way forward is to conceive of the census
procedure as a measure of a latent variable. With that tack, we can
evaluate MSA's by comparing the shape we 


Using a psychometric strategy for establishing the
validity of measurements of latent variables, and a metropolitan area
is a latent variable, we could compare MSA with other partitions of
space that we created using other features we believe associated with
metropolitan areas and which are not just proxies for number of
commuters.

For One rarely used strategy is to ask people how they would
classify the world.





First, we may look to at test-rest reliability. I

First, we 
may ask if it is reliable


In evaluating such classifications, we cannot use our standard
techniques for assessing reliability. 

....


However, assessing the reliability of such 


In either case, 



 






is only one groupings, like
schools, prisons, courtrooms,









school, the recent work of Raudenbush can guide us devising and
evaluating measures of attributes of a grouping, say teacher ability
and school effects. However, the if




hology and survey research has built up a 
Because of



the relatively little predictive power

We have to convince ourselves
and also our colleagues that our data bears upon our argument.


Even the most holistic amongst us, when
confront behavior and traces of behavior, we

Historically, this has been
very difficult.

When we study behavior and traces of behavior, we are often in the
position of trying to decide when a collection of grains of sand
becomes a pile. Say we want to study a group process, we will want to
identify all the members of the group. But there is no group, there is
only



If we want to study an organization, we have to
identify, at least implicitly, the things that 

identify a subset of all the things we
could possible attend to that

separate those parts of the world that we consider to be important
parts of the organization from those that we do not. Perhaps 










organize the world into 
organize some set of observable features
into an instance. 

our world that

to define what part of the world they hope
to understand. 
Even the most committed holist must study some finite
thing or set of things due to limits of time, money, and the patience
of dissertation committees.

Ideally, 

Unfortunately, for most of the phenomena we study, finding an obvious
and uncontroversial 


are notoriously
difficult to define 


to study some
bounded thing if only because we have to 



scientists face is to define our The first and problem One of the most
vexed problems faced by empirical social scientists is defining their
case.


<<KSplot1, fig=TRUE, pdf=TRUE, tikz=FALSE, eval=TRUE, height=4.75, eval=FALSE>>=
simplePlot(range)


plotPredictions(C, colors, 100000, range, boundaries=FALSE)


neighborhoods <- neighborhoods[neighborhoods != "southport corridor"]
text(hood.medoids[match(neighborhoods, hood.medoids$neighborhood),
                  c('x', 'y')],
     labels = neighborhoods,
     col= "#00000063",
     cex=.3)
text(hood.medoids[match("southport corridor", 
                        hood.medoids$neighborhood),
                  c('x', 'y')],
     labels = "southport corridor",
     col= "#00000063",
     cex=.3,
     srt=90)

@
